import numpy as np
import argparse
import sys
import shutil
import pyonmttok
import os
import glob
import math
from tqdm import tqdm

def createFolders(args):
    required_dirnames = [
        "data",
        "Outputs",
        "temp",
        "Preprocessed",
        "Reports",
        "Models"
    ]

    # do cleanup first
    for dirname in required_dirnames[:3]:
        if os.path.isdir(os.path.join(args.output_dir, dirname)):
            shutil.rmtree(os.path.join(args.output_dir, dirname))

    if args.do_preprocess and os.path.isdir(
        os.path.join(args.output_dir, "Preprocessed")
    ):
        shutil.rmtree(os.path.join(args.output_dir, "Preprocessed"))

    for dirname in required_dirnames:
        os.makedirs(os.path.join(args.output_dir, dirname), exist_ok=True)

def _merge(args, data_type):
    with (open(os.path.join(args.output_dir, "data", f"src-{data_type}.txt"), 'w') as srcF, open(os.path.join(args.output_dir, "data", f"tgt-{data_type}.txt"), 'w') as tgtF):
        
        for src_file in glob.glob(os.path.join(args.input_dir, "data", f"*.{data_type}.{args.src_lang}")):
            tgt_file_prefix = f'{src_file.rsplit(f".{data_type}.{args.src_lang}", 1)[0]}.{data_type}.{args.tgt_lang}'
            if tgt_files := glob.glob(f"{tgt_file_prefix}*"):
                # when multiple references are present, pick the first one
                tgt_file = tgt_files[0]

                with open(src_file) as f:
                    for line in f:
                        print(line.strip(), file=srcF)

                with open(tgt_file) as f:
                    for line in f:
                        print(line.strip(), file=tgtF)

def _move(args, dataset_category):
    for src_file in glob.glob(os.path.join(args.input_dir, "data", f"*.{dataset_category}.{args.src_lang}")):
        tgt_file_prefix = f'{src_file.rsplit(f".{dataset_category}.{args.src_lang}", 1)[0]}.{dataset_category}.{args.tgt_lang}'
        if tgt_files := glob.glob(f"{tgt_file_prefix}*"):
            shutil.copy(
                src_file,
                os.path.join(
                    args.output_dir,
                    "Outputs",
                    f".src-{dataset_category}.txt".join(
                        os.path.basename(src_file).rsplit(f".{dataset_category}.{args.src_lang}", 1)
                    )
                ) 
            )
            for tgt_file in tgt_files:
                shutil.copy(
                tgt_file, 
                os.path.join(
                    args.output_dir,
                    "Outputs",
                    f".tgt-{dataset_category}.txt".join(
                        os.path.basename(tgt_file).rsplit(f".{dataset_category}.{args.tgt_lang}", 1)
                    )
                )
            )
    
def moveRawData(args):
    # move vocab models
    shutil.copy(
        os.path.join(args.input_dir, "vocab", f"{args.src_lang}.model"),
        os.path.join(args.output_dir, "Preprocessed", "srcSPM.model")
    )
    shutil.copy(
        os.path.join(args.input_dir, "vocab", f"{args.tgt_lang}.model"),
        os.path.join(args.output_dir, "Preprocessed", "tgtSPM.model")
    )

    vocab_cmd = [
        "spm_export_vocab --model",
        os.path.join(args.output_dir, "Preprocessed", "srcSPM.model"),
        "| cut -f1 >",
        os.path.join(args.output_dir, "Preprocessed", "srcSPM.vocab")
    ]
    os.system(" ".join(vocab_cmd))

    vocab_cmd = [
        "spm_export_vocab --model",
        os.path.join(args.output_dir, "Preprocessed", "tgtSPM.model"),
        "| cut -f1 >",
        os.path.join(args.output_dir, "Preprocessed", "tgtSPM.vocab")
    ]
    os.system(" ".join(vocab_cmd))

    if args.do_train and args.do_preprocess:
        _merge(args, "train")
        _merge(args, "valid")

        if not glob.glob(os.path.join(args.input_dir, "data", f"*.valid.{args.src_lang}")):       
            np.random.seed(3435)
            sampledCount = 0

            with (open(os.path.join(args.output_dir, "data", "src-train.txt.backup"), 'w') as srcT, open(os.path.join(args.output_dir, "data", "tgt-train.txt.backup"), 'w') as tgtT, open(os.path.join(args.output_dir, "data", "src-valid.txt"), 'w') as srcV, open(os.path.join(args.output_dir, "data", "tgt-valid.txt"), 'w') as tgtV, open(os.path.join(args.output_dir, "data", "src-train.txt")) as srcO, open(os.path.join(args.output_dir, "data", "tgt-train.txt")) as tgtO):
                
                for srcLine, tgtLine in zip(srcO, tgtO):
                    if (
                        sampledCount < args.validation_samples
                        and np.random.random() > 0.5
                    ):
                        print(srcLine.strip(), file=srcV)
                        print(tgtLine.strip(), file=tgtV)
                        sampledCount += 1
                        continue

                    print(srcLine.strip(), file=srcT)
                    print(tgtLine.strip(), file=tgtT)

            shutil.move(
                os.path.join(args.output_dir, "data", "src-train.txt.backup"),
                os.path.join(args.output_dir, "data", "src-train.txt")
            )
            shutil.move(
                os.path.join(args.output_dir, "data", "tgt-train.txt.backup"),
                os.path.join(args.output_dir, "data", "tgt-train.txt")
            )


    if args.do_eval:
        _move(args, "valid")
        _move(args, "test")
        
def _lc(input_file):
    lc = 0
    with open(input_file) as f:
        for _ in f:
            lc += 1
    return lc
    

def spmOperate(args, fileType, tokenize):
    if tokenize:
        modelName = os.path.join(args.output_dir, "Preprocessed", f"{fileType}SPM.model")
        input_files = (
            glob.glob(os.path.join(args.output_dir, "data", f'*{fileType}-*')) + 
            glob.glob(os.path.join(args.output_dir, "Outputs", f'*{fileType}-*'))
        )

        for input_file in input_files:
            if '-train.txt' in input_file:
                for e in tqdm(range(64), desc=f"Applying regularization on {os.path.basename(input_file)}"):
                    seed_cmd = [
                        f"seq 1 {max(10000, _lc(input_file))}",
                        "|",
                        f"openssl enc -base64 -pass pass:{e} -nosalt 2>/dev/null",
                        f"-out \"{os.path.join(args.output_dir, 'temp', 'seed.txt')}\""
                    ]
                    os.system(" ".join(seed_cmd))
                    spm_cmd = [
                        f"spm_encode --model=\"{modelName}\"",
                        f"--output_format=sample_piece --nbest_size={args.nbest} --alpha={args.alpha}",
                        f"< \"{input_file}\" |",
                        f"shuf --random-source=\"{os.path.join(args.output_dir, 'temp', 'seed.txt')}\"",
                        f">> \"{input_file}.tok\""
                    ]
                    os.system(" ".join(spm_cmd))
                    os.remove(os.path.join(args.output_dir, 'temp', 'seed.txt'))
            else:
                spm_cmd = [
                    f'spm_encode --model=\"{modelName}\"',
                    "--output_format=piece",
                    f'< \"{input_file}\" > \"{input_file}.tok\"',
                ]
                os.system(" ".join(spm_cmd))

            os.remove(input_file)
    else:
        modelName = os.path.join(args.output_dir, "Preprocessed", "tgtSPM.model")
        for input_file in glob.glob(os.path.join(args.output_dir, "Outputs", f'*{fileType}-*.tok')):
            spm_cmd = [
                f"spm_decode --model=\"{modelName}\"",
                f"< \"{input_file}\" > \"{'.detok'.join(input_file.rsplit('.tok', 1))}\""
            ]
            os.system(" ".join(spm_cmd))
            os.remove(input_file)
            post_cmd = f"""sed 's/‚ñÅ/ /g;s/  */ /g' -i \"{'.detok'.join(input_file.rsplit('.tok', 1))}\""""
            os.system(post_cmd)
        
        
def tokenize(args):
    spmOperate(args, 'src', tokenize=True)
    spmOperate(args, 'tgt', tokenize=True)
            
def detokenize(args):        
    spmOperate(args, 'tgt', tokenize=False)
    spmOperate(args, 'pred', tokenize=False)

def processData(args, tokenization):
    if tokenization:
        createFolders(args)
        moveRawData(args)
        tokenize(args)
    else:
        detokenize(args)

